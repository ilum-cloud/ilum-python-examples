{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<center><img src=\"images/logo2.png\"/></center>\n",
    "\n",
    "<center><h1>Sedona intro</h1></center>\n",
    "<center><h3><a href = 'https://ilum.cloud'>ilum.cloud</a></h3></center>\n",
    "<center>Welcome to the Ilum Interactive Capabilities Tutorial! In this section you will learn how to start processing spartial data with Apache Sedona on the Ilum. Let's dive in!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Sedona** is a distributed system for processing large-scale spatial data. It extends existing cluster computing systems, such as Apache Spark and Apache Flink, with a set of out-of-the-box distributed Spatial Datasets and Spatial SQL that efficiently load, process, and analyze large-scale spatial data across machines.\n",
    "\n",
    "Key features of Apache Sedona include:\n",
    "\n",
    "- Support for a wide range of spatial data formats, including GeoJSON, SHP, GPX, and GeoParquet.\n",
    "- Efficient spatial operations, such as distance calculations, spatial joins, and aggregations.\n",
    "- Scalability to handle large-scale spatial datasets.\n",
    "- Ease of use, with a simple and intuitive API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- [Ilum Documentation](https://ilum.cloud/docs)\n",
    "- [Apache Sedona Documentation](https://sedona.apache.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "  <b>Warning:</b> Before creating a session, ensure the following:\n",
    "  <ul>\n",
    "    <li>\n",
    "      The Spark image is set to <b>ilum/spark:3.5.0-sedona-1.6.0</b> in your cluster settings.\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.6.0-31.0/geotools-wrapper-1.6.0-31.0.jar\">geotools-wrapper-1.6.0-31.0.jar</a> <a>are in the ilum-files/jars bucket</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.5_2.12/1.6.0/sedona-spark-shaded-3.5_2.12-1.6.0.jar\">sedona-spark-shaded-3.5_2.12-1.6.0.jar</a> <a>are in the ilum-files/jars bucket</a>\n",
    "        </li>\n",
    "      </ul>\n",
    "    </li>\n",
    "  </ul>\n",
    "  <p>All the above steps are described in detail here: <a href=\"https://docs.google.com/document/d/1fV7k7K8UOP1LnIzNPlXM8l0Tot3KgA60uowBWYkLXxI/edit\">Apache Sedona with Ilum (link do zmiany kiedy wydamy bloga)</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To start interacting with the remote cluster, we'll need to load the spark magic extension. You can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we'll need to set up an endpoint. An endpoint is simply a URL that points to a specific Spark cluster. You can choose scala or python, but we will focus on python here. You can do this by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example properties**:\\\n",
    "{\"conf\": {\"spark.sql.extensions\": \"org.apache.sedona.sql.SedonaSqlExtensions\", \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\", \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\", \"spark.jars\": \"s3a://ilum-files/jars/geotools-wrapper-1.6.0-31.0.jar,s3a://ilum-files/jars/sedona-spark-shaded-3.5_2.12-1.6.0.jar\"},\"driverMemory\": \"1000M\", \"executorCores\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Fueling the Geospatial Analysis Engine: Uploading Data**\n",
    "Before we unleash the awesome power of Sedona for geospatial analysis, we need some fuel: data! In this case, we'll need two specific datasets:\n",
    "\n",
    "1. **ne_50m_admin_0_countries_lakes.zip**: This file contains data on countries and lakes, providing the geographical context for our analysis.\n",
    "2. **ne_50m_airports.zip**: This file holds information on airports, perfect for exploring potential travel routes or analyzing airport distribution.\n",
    "\n",
    "The data is readily available for download [here](https://github.com/ilum-cloud/ilum-python-examples/tree/main/sedona-example). Head over to the Data section. Once you're there, create a new folder named \"geodata\" within the ilum-files bucket. With that folder ready, it's upload time! Transfer the two data files into the newly created folder called \"geodata\" within the ilum-files bucket.\n",
    "\n",
    "Now that we've fed our data pipeline, we're ready to leverage Sedona's geospatial magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start processing, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    from sedona.spark import *\n",
    "    import geopandas as gpd\n",
    "    import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another necessary step before starting to process spatial data is SedonaContext initialization using the SparkSession (available as spark).  Think of it as the secret handshake that lets Spark understand the language of spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use a different way of accessing the data due to its original structure. The following credentials are set by default when building Ilum. If the default credentials don't work, contact your Ilum platform administrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "          key='minioadmin',\n",
    "          secret='minioadmin',\n",
    "          endpoint_url='http://ilum-minio:9000/'\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the download of the files from the bucket, packing them into the geopandas dataframe format and creating a Temporary View."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    countries_gpd = gpd.read_file(s3.open('s3://ilum-files/geodata/ne_50m_admin_0_countries_lakes.zip'), engine='pyogrio')\n",
    "    countries_df = spark.createDataFrame(countries_gpd)\n",
    "    countries_df.createOrReplaceTempView(\"country\")\n",
    "    countries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    airports_gpd = gpd.read_file(s3.open('s3://ilum-files/geodata/ne_50m_airports.zip'), engine='pyogrio')\n",
    "    airports_df = spark.createDataFrame(airports_gpd)\n",
    "    airports_df.createOrReplaceTempView(\"airport\")\n",
    "    airports_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sedona enables data processing spatial through:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***SQL API***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've imported the data, you can create a spatial join using ***SQL API***. In this case, we want to match airports with countries based on their location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    result = spark.sql(\"SELECT c.geometry as country_geom, c.NAME_EN, a.geometry as airport_geom, a.name FROM country c, airport a WHERE ST_Contains(c.geometry, a.geometry)\")\n",
    "    result.createOrReplaceTempView(\"result\")\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***RDD API***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below we convert the data into special Spark DataFrames called SpatialRDDs using the Sedona Adapter, enabling spatial operations based on their \"geometry\" columns. Leveraging these spatial optimizations, we perform a spatial join between the airports and countries SpatialRDDs to find matching locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    airports_rdd = Adapter.toSpatialRdd(airports_df, \"geometry\")\n",
    "    # Drop the duplicate name column in countries_df\n",
    "    countries_df = countries_df.drop(\"NAME\")\n",
    "    countries_rdd = Adapter.toSpatialRdd(countries_df, \"geometry\")\n",
    "    \n",
    "    airports_rdd.analyze()\n",
    "    countries_rdd.analyze()\n",
    "    \n",
    "    # 4 is the num partitions used in spatial partitioning. This is an optional parameter\n",
    "    airports_rdd.spatialPartitioning(GridType.KDBTREE, 4)\n",
    "    countries_rdd.spatialPartitioning(airports_rdd.getPartitioner())\n",
    "    \n",
    "    buildOnSpatialPartitionedRDD = True\n",
    "    usingIndex = True\n",
    "    considerBoundaryIntersection = True\n",
    "    airports_rdd.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n",
    "    \n",
    "    result_pair_rdd = JoinQueryRaw.SpatialJoinQueryFlat(airports_rdd, countries_rdd, usingIndex, considerBoundaryIntersection)\n",
    "    \n",
    "    result2 = Adapter.toDf(result_pair_rdd, countries_rdd.fieldNames, airports_rdd.fieldNames, spark)\n",
    "    \n",
    "    result2.createOrReplaceTempView(\"join_result_with_all_cols\")\n",
    "    # Select the columns needed in the join\n",
    "    result2 = spark.sql(\"SELECT leftgeometry as country_geom, NAME_EN, rightgeometry as airport_geom, name FROM join_result_with_all_cols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print spatial join results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "    \n",
    "    print(\"The result of SQL API\")\n",
    "    result.show()\n",
    "    print(\"The result of RDD API\")\n",
    "    result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll group airports based on the country they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    groupedresult = spark.sql(\"SELECT c.NAME_EN, c.country_geom, count(*) as airports_count FROM result c GROUP BY c.NAME_EN, c.country_geom\")\n",
    "    groupedresult.createOrReplaceTempView(\"groupedresult\")\n",
    "    groupedresult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of processing will be to upload the processed data to a selected database for visualization or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "    \n",
    "    groupedresult.write.mode('overwrite').format(\"geoparquet\").save(\"s3a://ilum-files/geodata/airports.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of data visualization, let's present them in a convenient combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o visualresult_df\n",
    "\n",
    "    # Join the tables with proper column aliases\n",
    "    visual_data = spark.sql(\"\"\"\n",
    "    SELECT a.NAME_EN AS country_name, \n",
    "           a.name AS airport_name, \n",
    "           a.airport_geom, \n",
    "           c.airports_count\n",
    "    FROM result AS a\n",
    "    JOIN groupedresult AS c ON a.NAME_EN = c.NAME_EN\n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert Spark DataFrame to GeoPandas DataFrame\n",
    "    gdf = gpd.GeoDataFrame(visual_data.toPandas(), geometry='airport_geom')\n",
    "    \n",
    "    # Extract longitude and latitude coordinates from the geometry\n",
    "    gdf['longitude'] = gdf['airport_geom'].x\n",
    "    gdf['latitude'] = gdf['airport_geom'].y\n",
    "    \n",
    "    # Drop the 'airport_geom' column\n",
    "    gdf = gdf.drop('airport_geom', axis=1)\n",
    "    \n",
    "    # Create a new Spark DataFrame from the GeoPandas DataFrame\n",
    "    visualresult_df = spark.createDataFrame(gdf)\n",
    "    visualresult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "  <b>Warning:</b> Visualization processing is occurring within a different kernel!\n",
    "  <ul>\n",
    "    <li>\n",
    "      Due to the limitations of the kernel we are using for computation, advanced visualization methods are not available. Therefore, all visualizations will be performed within the ipykernel environment.\n",
    "    </li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "Preprocessed data will be exchanged between kernels in the form of DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with pygwalker in the ipykernel kernel, you'll need to install the following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas pygwalker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the enviroment is prepared, you can simply call a visualization tool, passing your data frame as an argument. For detailed guidance on setting parameters for optimal visualization, please refer to our [blog post](https://sedona.apache.org/) (link do zmiany)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import pygwalker as pyg\n",
    "\n",
    "    walker = pyg.walk(visualresult_df, kernel_computation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "\n",
    "Now that you’re done with your work, you should clean them up.\n",
    "Simply click on the Delete buttons!\n",
    "\n",
    "![Ilum session clean](../../images/clean_ilum_jupyter_session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
