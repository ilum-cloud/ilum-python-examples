{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://ilum.cloud\"><img src=\"../logo.svg\" alt=\"ILUM Logo\"></a></center>\n",
    "\n",
    "<center><h1 style=\"padding-left: 32px;\">Bronze to Silver</h1></center>\n",
    "<center>Welcome to the Ilum Interactive Capabilities Tutorial! In this section, you can transform the data from the bronze layer to meet the assumptions of the silver layer. Let's dive in!</center>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Silver Layer\n",
    "\n",
    "The **Silver Layer** is the middle layer of the Medallion architecture. It stores cleansed and conformed data from the Bronze Layer, making it ready for analysis and other downstream applications.\n",
    "\n",
    "#### **Typical Data in the Silver Layer**\n",
    "The data stored in the Silver Layer often includes:\n",
    "- **Cleansed Data**: Data from the Bronze Layer that has been cleansed of errors and inconsistencies.\n",
    "- **Conformed Data**: Data standardized to a common schema across various sources.\n",
    "- **Enriched Data**: Data enhanced with additional information, such as historical or demographic data.\n",
    "\n",
    "This data is typically stored in relational databases, data warehouses, or other cloud-based data lakes.\n",
    "\n",
    "#### **Purpose of the Silver Layer**\n",
    "The Silver Layer serves several important purposes:\n",
    "- **Clean and Conformed View**: Provides a consistent and error-free representation of the data.\n",
    "- **Accessibility**: Makes data easily available for analysis and downstream applications.\n",
    "- **Foundation for the Gold Layer**: Acts as a starting point for further transformations into refined datasets.\n",
    "\n",
    "#### **Applications of Silver Layer Data**\n",
    "Data from the Silver Layer can be used for:\n",
    "- Trend and customer behavior analysis.\n",
    "- Identifying opportunities to improve efficiency.\n",
    "- Supporting business decision-making processes.\n",
    "\n",
    "#### **Summary**\n",
    "The **Silver Layer** plays a critical role in the Medallion architecture by refining and enriching raw data from the Bronze Layer. Through processes like deduplication, filtering, and transformation, the Silver Layer produces clean, structured, and analyzable datasets. This layer bridges the gap between raw data and actionable insights, ensuring consistency and reliability for downstream applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a continuation, let's now walk through an example of processing and enriching data in the Silver Layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Transforming Data into the Silver Layer\n",
    "\n",
    "In this example, we will demonstrate how to transform data from the Bronze Layer into the Silver Layer by cleansing, conforming, and enriching it for analytical use cases.\n",
    "\n",
    "#### **Step 1: Set Up the Environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we'll need to load the spark magic extension. You can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ilum's Bundled Jupyter is ready to work out of the box and has a predefined endpoint address, which points to ```livy-proxy```. \n",
    "\n",
    "Use **%manage_spark** to create new session. \n",
    "\n",
    "Choose between Scala or Python, adjust Spark settings if necessary, and then click the `Create Session` button. As simple as that. \n",
    "\n",
    "The following example is written in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we start processing, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    from pyspark.sql.functions import to_date, col\n",
    "    from pyspark.sql.types import IntegerType, StringType, LongType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Dedicated Database for the Use Case**\n",
    "\n",
    "A good practice in data engineering is to separate data within dedicated databases for specific use cases. This approach helps maintain data organization and makes it easier to manage, query, and scale.\n",
    "\n",
    "For this use case, we will create a database named `example_silver`. This will ensure that all data related to this use case is stored in a structured and isolated manner.\n",
    "\n",
    "To create the database, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    spark.sql(\"CREATE DATABASE example_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Load Data from the Bronze Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second stage of processing in this layer is to read data from the bronze layer, set the correct data types and reject invalid rows. The operation is repeated for each data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### **animals**\n",
    "We start by reading the `animals` table from the `example_bronze` database. To ensure data cleanliness, we use the `dropna()` method to remove rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    animals_bronze_df = spark.read.table(\"example_bronze.animals\").dropna()\n",
    "    animals_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"owner_id\", IntegerType(), False),\n",
    "        StructField(\"specie_id\", IntegerType(), False),\n",
    "        StructField(\"animal_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"birth_date\", StringType(), False),\n",
    "        StructField(\"color\", StringType(), False),\n",
    "        StructField(\"size\", StringType(), False),\n",
    "        StructField(\"weight\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    animals_df = spark.createDataFrame(animals_bronze_df.rdd, schema=animals_schema)\n",
    "    animals_df.printSchema()\n",
    "    animals_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:51:29.296345Z",
     "iopub.status.busy": "2024-12-09T14:51:29.295995Z",
     "iopub.status.idle": "2024-12-09T14:51:29.306137Z",
     "shell.execute_reply": "2024-12-09T14:51:29.304320Z",
     "shell.execute_reply.started": "2024-12-09T14:51:29.296305Z"
    }
   },
   "source": [
    "The resulting `animals_df` contains data that adheres to the specified schema. This ensures consistency and reliability for downstream processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **owners**\n",
    "This time, we will walk through the entire process for the `owners` table, including data reading, schema refinement, and preparing it future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    owners_bronze_df = spark.read.table(\"example_bronze.owners\").dropna()\n",
    "    owners_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    owners_schema = StructType([\n",
    "                        StructField(\"owner_id\", IntegerType(), False),\n",
    "                        StructField(\"first_name\", StringType(), False),\n",
    "                        StructField(\"last_name\", StringType(), False),\n",
    "                        StructField(\"mobile\", LongType(), False),\n",
    "                        StructField(\"email\", StringType(), False)\n",
    "                        ])\n",
    "\n",
    "    owners_df = spark.createDataFrame(owners_bronze_df.rdd, schema=owners_schema)\n",
    "    owners_df.printSchema()\n",
    "    owners_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:50:39.690179Z",
     "iopub.status.busy": "2024-12-05T15:50:39.689018Z",
     "iopub.status.idle": "2024-12-05T15:50:39.702567Z",
     "shell.execute_reply": "2024-12-05T15:50:39.701197Z",
     "shell.execute_reply.started": "2024-12-05T15:50:39.690139Z"
    }
   },
   "source": [
    " - #### **species**\n",
    "This time, we will walk through the entire process for the `species` table, including data reading, schema refinement, and preparing it future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    species_bronze_df = spark.read.table(\"example_bronze.species\").dropna()\n",
    "    species_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    species_schema = StructType([\n",
    "                        StructField(\"specie_id\", IntegerType(), False),\n",
    "                        StructField(\"specie_name\", StringType(), False)\n",
    "                        ])\n",
    "\n",
    "    species_df = spark.createDataFrame(species_bronze_df.rdd, schema=species_schema)\n",
    "    species_df.printSchema()\n",
    "    species_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Transform and Cleanse Data**\n",
    "The third stage of processing data from the brown layer will be combining them in the result table and formatting the data.\n",
    "Below two Dataframes are combined to link each animal to its corresponding species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    animals_df = animals_df. \\\n",
    "    join(species_df, animals_df[\"specie_id\"] == species_df[\"specie_id\"], 'left'). \\\n",
    "    select(animals_df[\"id\"], \\\n",
    "           animals_df[\"owner_id\"], \\\n",
    "           species_df[\"specie_name\"], \\\n",
    "           animals_df[\"animal_name\"], \\\n",
    "           to_date(animals_df['birth_date'],'MM/dd/yyyy').alias('birth_date'), \\\n",
    "           animals_df[\"gender\"], \\\n",
    "           animals_df[\"size\"], \\\n",
    "           animals_df[\"color\"], \\\n",
    "           animals_df[\"weight\"], \\\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Save Data to the Silver Layer**\n",
    "Save the cleansed and conformed data to the Silver Layer in Delta format. \\\n",
    "The use of the delta format in this case allows access to the history of changes and optimizes the amount of memory consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_df.write.format(\"delta\").saveAsTable(\"example_silver.animals\")\n",
    "    owners_df.write.format(\"delta\").saveAsTable(\"example_silver.owners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary**\n",
    "In this example:\n",
    "\n",
    " - **We loaded data** from the Bronze Layer.\n",
    " - **We transformed the data** by cleansing it of errors and conforming it to a consistent schema.\n",
    " - **We used SQL to join the `owners` and `animals` tables**, enriching the data by combining relevant information from both sources.\n",
    " - **We saved the processed data** to the Silver Layer in Delta format for easy accessibility.\n",
    "\n",
    "This structured approach ensures that data is ready for analysis and supports efficient business decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "\n",
    "Now that you’re done with your work, you should clean them up to free up resources when they’re no longer in use. \n",
    "Simply click on the Delete buttons!\n",
    "\n",
    "![Ilum session clean](../../images/clean_ilum_jupyter_session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Click here to proceed to the \"Silver to gold\" section.](3_Silver_to_gold.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
