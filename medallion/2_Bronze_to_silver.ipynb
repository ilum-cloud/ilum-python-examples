{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://ilum.cloud\"><img src=\"../logo.svg\" alt=\"ILUM Logo\"></a></center>\n",
    "\n",
    "<center><h1 style=\"padding-left: 32px;\">Bronze to Silver</h1></center>\n",
    "<center>Welcome to the Ilum Interactive Capabilities Tutorial! In this section, you can transform the data from the bronze layer to meet the assumptions of the silver layer. Let's dive in!</center>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Silver Layer\n",
    "\n",
    "The **Silver Layer** is the intermediate tier of the Medallion architecture, responsible for refining raw data from the Bronze Layer into cleansed, structured, and standardized datasets. By applying data quality transformations, deduplication, and schema conformance rules, this layer ensures that data is accurate, consistent, and ready for analytical and operational use.\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "- **Data Cleansing and Standardization:**  \n",
    "  The Silver Layer removes errors, inconsistencies, duplicates, and addresses missing or invalid values. Data undergoes validation against business-specific rules to ensure accuracy and reliability.\n",
    "\n",
    "- **Schema Harmonization and Conformance:**  \n",
    "  Unifies data from various sources by aligning field names, data types, and formats into a standardized schema, making it consistent and coherent across different systems.\n",
    "\n",
    "- **Enrichment and Augmentation:**  \n",
    "  Enhances datasets by adding contextual information such as demographics, historical records, or reference data, improving analytical depth and value.\n",
    "\n",
    "- **Data Filtering and Transformation:**  \n",
    "  Applies business logic and structured transformations, converting raw data into refined, analytics-ready datasets. Irrelevant or redundant data is filtered out, and essential data is transformed for clarity and usability.\n",
    "\n",
    "- **Incremental Processing and Change Tracking:**  \n",
    "  Implements incremental update methods such as Slowly Changing Dimensions (SCD) and Change Data Capture (CDC) to efficiently handle data updates and maintain historical accuracy, supporting effective time-based analytics.\n",
    "\n",
    "- **Data Governance and Lineage:**  \n",
    "  Maintains comprehensive metadata and lineage tracking, capturing the origin of data, transformations performed, and historical modifications. This transparency supports auditing, governance, and regulatory compliance.\n",
    "\n",
    "## Why Use the Silver Layer?\n",
    "\n",
    "The Silver Layer plays a crucial role in converting raw data into reliable, structured information suitable for business analysis. Its benefits include:\n",
    "\n",
    "- **Improved Data Quality:**  \n",
    "  By systematically cleansing and standardizing data, it significantly reduces potential errors and enhances the trustworthiness of analytics and decision-making processes.\n",
    "\n",
    "- **Efficient Data Consumption:**  \n",
    "  Structured and standardized data simplifies queries and reduces processing overhead, enabling faster and easier access for analysts, data scientists, and business users.\n",
    "\n",
    "- **Enhanced Business Insights:**  \n",
    "  Clean and enriched data facilitates deeper, more accurate analyses, supporting advanced analytics, predictive modeling, and strategic decision-making.\n",
    "\n",
    "- **Operational Readiness:**  \n",
    "  The refined data in the Silver Layer is readily consumable by operational systems and analytical tools without additional preprocessing, enabling rapid deployment into dashboards, BI tools, or production applications.\n",
    "\n",
    "- **Regulatory Compliance:**  \n",
    "  By ensuring consistent data quality, structured metadata, and transparent lineage tracking, the Silver Layer facilitates compliance with industry regulations such as GDPR, HIPAA, and others, simplifying audits and governance activities.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The **Silver Layer** is a critical component within the Medallion architecture, bridging the gap between raw data ingestion in the Bronze Layer and advanced analytics performed in the Gold Layer. By cleansing, standardizing, enriching, and structuring data, it ensures that organizations have reliable, consistent, and actionable datasets. This robust intermediate layer empowers business users and analysts with accurate insights, driving informed decision-making and streamlined analytics workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a continuation, let's now walk through an example of cleaning and enriching data in the Silver Layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Transforming Data into the Silver Layer\n",
    "\n",
    "In this example, we will demonstrate how to transform data from the Bronze Layer into the Silver Layer by cleansing, conforming, and enriching it for analytical use cases.\n",
    "\n",
    "### **Step 1: Set Up the Environment**\n",
    "To begin, we need to ensure our environment is ready for data processing and Delta - Hive integration. This includes setting up any necessary configurations, and importing all the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <h4 class=\"alert-heading\">Before running your Delta notebook</h4>\n",
    "  <p>Please ensure your environment is properly configured for Delta Lake and Hive integration.</p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      <strong>Global Delta Capabilities:</strong> Ensure that your cluster or global Spark configuration includes the following settings:\n",
    "      <table class=\"table table-bordered\" style=\"text-align: left;\">\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th>key</th>\n",
    "            <th>value</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "          <tr>\n",
    "            <td>spark.sql.extensions</td>\n",
    "            <td>io.delta.sql.DeltaSparkSessionExtension</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.sql.catalog.spark_catalog</td>\n",
    "            <td>org.apache.spark.sql.delta.catalog.DeltaCatalog</td>\n",
    "          </tr>\n",
    "           <tr>\n",
    "            <td>spark.databricks.delta.catalog.update.enabled</td>\n",
    "            <td>true</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.kubernetes.container.image</td>\n",
    "            <td>ilum/spark:3.5.3-delta</td>\n",
    "          </tr>\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>Hive Integration Requirements:</strong> This notebook is integrated with Hive. To properly support Hive, you must enable Hive in your environment. For detailed instructions, please refer to <a href=\"https://ilum.cloud/resources/getting-started\" target=\"_blank\" rel=\"noopener noreferrer\">this guide</a>. Also, add the following properties to your cluster configuration:\n",
    "      <table class=\"table table-bordered\" style=\"text-align: left;\">\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th>key</th>\n",
    "            <th>value</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "          <tr>\n",
    "            <td>spark.hadoop.hive.metastore.uris</td>\n",
    "            <td>thrift://ilum-hive-metastore:9083</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.sql.catalogImplementation</td>\n",
    "            <td>hive</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.sql.warehouse.dir</td>\n",
    "            <td>s3a://ilum-data/</td>\n",
    "          </tr>\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>Session-Specific Delta-Hive Capabilities:</strong> If Delta and Hive is only required for a specific session, configure the necessary environment variables and dependencies on a per-session basis. For example:\n",
    "      <pre><code>{\"conf\": {\"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\", \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\", \"spark.sql.warehouse.dir\": \"s3a://ilum-data/\", \"spark.kubernetes.container.image\": \"ilum/spark:3.5.3-delta\", \"spark.databricks.delta.catalog.update.enabled\": \"true\", \"spark.hadoop.hive.metastore.uris\": \"thrift://ilum-hive-metastore:9083\", \"spark.sql.catalogImplementation\": \"hive\"}, \"driverMemory\": \"1000M\", \"executorCores\": 2}</code></pre>\n",
    "      This configuration prepares your session for Delta operations without affecting other workflows.\n",
    "    </li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we'll need to load the spark magic extension. You can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ilum's Bundled Jupyter is ready to work out of the box and has a predefined endpoint address, which points to ```livy-proxy```. \n",
    "\n",
    "Use **%manage_spark** to create new session. \n",
    "\n",
    "Choose between Scala or Python, adjust Spark settings if necessary, and then click the `Create Session` button. As simple as that. \n",
    "\n",
    "The following example is written in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we start processing, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    from pyspark.sql.functions import to_date, col\n",
    "    from pyspark.sql.types import IntegerType, StringType, LongType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Dedicated Database for the Use Case**\n",
    "\n",
    "A good practice in data engineering is to separate data within dedicated databases for specific use cases. This approach helps maintain data organization and makes it easier to manage, query, and scale.\n",
    "\n",
    "For this use case, we will create a database named `example_silver`. This will ensure that all data related to this use case is stored in a structured and isolated manner.\n",
    "\n",
    "To create the database, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    spark.sql(\"CREATE DATABASE example_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Load Data from the Bronze Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second stage of processing in this layer is to read data from the bronze layer, set the correct data types and reject invalid rows. The operation is repeated for each data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### **animals**\n",
    "We start by reading the `animals` table from the `example_bronze` database. To ensure data cleanliness, we use the `dropna()` method to remove rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    animals_bronze_df = spark.read.table(\"example_bronze.animals\").dropna()\n",
    "    animals_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"owner_id\", IntegerType(), False),\n",
    "        StructField(\"specie_id\", IntegerType(), False),\n",
    "        StructField(\"animal_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"birth_date\", StringType(), False),\n",
    "        StructField(\"color\", StringType(), False),\n",
    "        StructField(\"size\", StringType(), False),\n",
    "        StructField(\"weight\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    animals_df = spark.createDataFrame(animals_bronze_df.rdd, schema=animals_schema)\n",
    "    animals_df.printSchema()\n",
    "    animals_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:51:29.296345Z",
     "iopub.status.busy": "2024-12-09T14:51:29.295995Z",
     "iopub.status.idle": "2024-12-09T14:51:29.306137Z",
     "shell.execute_reply": "2024-12-09T14:51:29.304320Z",
     "shell.execute_reply.started": "2024-12-09T14:51:29.296305Z"
    }
   },
   "source": [
    "The resulting `animals_df` contains data that adheres to the specified schema. This ensures consistency and reliability for downstream processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **owners**\n",
    "This time, we will walk through the entire process for the `owners` table, including data reading, schema refinement, and preparing it future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    owners_bronze_df = spark.read.table(\"example_bronze.owners\").dropna()\n",
    "    owners_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    owners_schema = StructType([\n",
    "                        StructField(\"owner_id\", IntegerType(), False),\n",
    "                        StructField(\"first_name\", StringType(), False),\n",
    "                        StructField(\"last_name\", StringType(), False),\n",
    "                        StructField(\"mobile\", LongType(), False),\n",
    "                        StructField(\"email\", StringType(), False)\n",
    "                        ])\n",
    "\n",
    "    owners_df = spark.createDataFrame(owners_bronze_df.rdd, schema=owners_schema)\n",
    "    owners_df.printSchema()\n",
    "    owners_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:50:39.690179Z",
     "iopub.status.busy": "2024-12-05T15:50:39.689018Z",
     "iopub.status.idle": "2024-12-05T15:50:39.702567Z",
     "shell.execute_reply": "2024-12-05T15:50:39.701197Z",
     "shell.execute_reply.started": "2024-12-05T15:50:39.690139Z"
    }
   },
   "source": [
    " - #### **species**\n",
    "This time, we will walk through the entire process for the `species` table, including data reading, schema refinement, and preparing it future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    species_bronze_df = spark.read.table(\"example_bronze.species\").dropna()\n",
    "    species_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Define and Enforce a Strict Schema**\n",
    "We define a strict schema using `StructType` to ensure that all columns have the correct data types. This step validates the data and makes the schema consistent across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    species_schema = StructType([\n",
    "                        StructField(\"specie_id\", IntegerType(), False),\n",
    "                        StructField(\"specie_name\", StringType(), False)\n",
    "                        ])\n",
    "\n",
    "    species_df = spark.createDataFrame(species_bronze_df.rdd, schema=species_schema)\n",
    "    species_df.printSchema()\n",
    "    species_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Transform and Cleanse Data**\n",
    "The third stage of processing data from the brown layer will be combining them in the result table and formatting the data.\n",
    "Below two Dataframes are combined to link each animal to its corresponding species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "\n",
    "    animals_df = animals_df. \\\n",
    "    join(species_df, animals_df[\"specie_id\"] == species_df[\"specie_id\"], 'left'). \\\n",
    "    select(animals_df[\"id\"], \\\n",
    "           animals_df[\"owner_id\"], \\\n",
    "           species_df[\"specie_name\"], \\\n",
    "           animals_df[\"animal_name\"], \\\n",
    "           to_date(animals_df['birth_date'],'MM/dd/yyyy').alias('birth_date'), \\\n",
    "           animals_df[\"gender\"], \\\n",
    "           animals_df[\"size\"], \\\n",
    "           animals_df[\"color\"], \\\n",
    "           animals_df[\"weight\"], \\\n",
    "          )\n",
    "\n",
    "    animals_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Save Data to the Silver Layer**\n",
    "Save the cleansed and conformed data to the Silver Layer in Delta format. \\\n",
    "The use of the delta format in this case allows access to the history of changes and optimizes the amount of memory consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_df.write.format(\"delta\").saveAsTable(\"example_silver.animals\")\n",
    "    owners_df.write.format(\"delta\").saveAsTable(\"example_silver.owners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "In this example:\n",
    "\n",
    " - **We loaded data** from the Bronze Layer.\n",
    " - **We transformed the data** by cleansing it of errors and conforming it to a consistent schema.\n",
    " - **We used SQL to join the `owners` and `animals` tables**, enriching the data by combining relevant information from both sources.\n",
    " - **We saved the processed data** to the Silver Layer in Delta format for easy accessibility.\n",
    "\n",
    "This structured approach ensures that data is ready for analysis and supports efficient business decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "\n",
    "Now that you’re done with your work, you should clean them up to free up resources when they’re no longer in use. \n",
    "Simply click on the Delete buttons!\n",
    "\n",
    "![Ilum session clean](../../images/clean_ilum_jupyter_session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Click here to proceed to the \"Silver to gold\" section.](3_Silver_to_gold.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
