{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://ilum.cloud\"><img src=\"../logo.svg\" alt=\"ILUM Logo\"></a></center>\n",
    "\n",
    "<center><h1 style=\"padding-left: 32px;\">Raw data to Bronze</h1></center>\n",
    "<center>Welcome to the Ilum Interactive Capabilities Tutorial! In this section you can load the first batch of data into the bronze layer. Let's dive in!</center>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bronze Layer\n",
    "\n",
    "The **Bronze Layer** is the foundational tier of a Medallion (Lakehouse) architecture, designed as a landing zone for raw, unprocessed data from multiple sources. It stores data exactly as it arrives from source systems such as databases, APIs, IoT sensors, logs, and more. This guarantees the integrity and completeness of incoming data before further processing or analysis.\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "- **Raw Data Storage:**  \n",
    "  Acts as a central repository for raw data in its original format, capturing information from operational systems (ERP, CRM, IoT devices, logs) without preprocessing. This approach ensures that all source data remains available for future use.\n",
    "\n",
    "- **Immutable Data Handling:**  \n",
    "  Data in the Bronze layer is typically stored as immutable, append-only records. This immutability preserves the original state of data, enabling auditing and exact reproducibility of historical data.\n",
    "\n",
    "- **Multi-Format Support:**  \n",
    "  Supports diverse data formats such as JSON, CSV, Parquet, Avro, and XML. Storing data in its original form allows handling semi-structured and unstructured datasets without upfront conversions.\n",
    "\n",
    "- **Schema Flexibility:**  \n",
    "  Utilizes a schema-on-read approach, applying schemas only at read-time. This flexibility accommodates evolving data structures and changing business requirements without disrupting ingestion.\n",
    "\n",
    "- **Efficient Data Ingestion:**  \n",
    "  Optimized for high-throughput ingestion from batch and streaming sources, allowing low-latency data capture. Minimal transformation during ingestion ensures data is loaded rapidly into the Bronze Layer.\n",
    "\n",
    "- **Data Lineage and Metadata Management:**  \n",
    "  Records metadata (e.g., ingestion timestamps, source identifiers, batch IDs) alongside data. This provides robust data lineage, facilitating governance, compliance, and troubleshooting.\n",
    "\n",
    "## Why Use the Bronze Layer?\n",
    "\n",
    "Raw data serves as a crucial foundation for advanced analytics and machine learning. The Bronze Layer provides key benefits, including:\n",
    "\n",
    "- **Data Reliability:**  \n",
    "  Preserving raw data as an unaltered source-of-truth enables reprocessing or re-analysis when downstream logic or requirements evolve.\n",
    "\n",
    "- **Scalability:**  \n",
    "  Designed to handle massive volumes of diverse data sources, the Bronze Layer supports continuous ingestion without performance bottlenecks, ensuring data availability for future use cases.\n",
    "\n",
    "- **Flexibility for Downstream Processing:**  \n",
    "  Acts as a staging area, allowing subsequent layers (Silver and Gold) to independently cleanse, integrate, and transform data as needed. Separation of ingestion from processing accelerates data onboarding and enhances analytical flexibility.\n",
    "\n",
    "- **Regulatory Compliance:**  \n",
    "  Maintaining original data with detailed lineage aids governance and traceability, essential for compliance with regulatory frameworks such as GDPR and HIPAA.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The Bronze Layer is an essential component of modern data lakehouse architectures. By reliably capturing and preserving raw data, it supports scalable, flexible, and compliant data management, laying the foundation for advanced analytics and informed decision-making in subsequent layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a continuation, let's now walk through an example of loading data into the **Bronze Layer**.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Loading Data into the Bronze Layer\n",
    "\n",
    "In this example, we will demonstrate how to load raw data into the Bronze Layer of the Medallion architecture. Letâ€™s assume we have a CSV file containing sales transaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Step 1: Set Up the Environment**\n",
    "To begin, we need to ensure our environment is ready for data processing and Hive integration. This includes setting up any necessary configurations, and importing all the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <h4 class=\"alert-heading\">Before running your notebook</h4>\n",
    "  <p>Please ensure your environment is properly configured for Hive integration.</p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      <strong>Hive Integration Requirements:</strong> This notebook is integrated with Hive. To properly support Hive, you must enable Hive in your environment. For detailed instructions, please refer to <a href=\"https://ilum.cloud/resources/getting-started\" target=\"_blank\" rel=\"noopener noreferrer\">this guide</a>. Also, add the following properties to your cluster configuration:\n",
    "      <table class=\"table table-bordered\" style=\"text-align: left;\">\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th>key</th>\n",
    "            <th>value</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "          <tr>\n",
    "            <td>spark.hadoop.hive.metastore.uris</td>\n",
    "            <td>thrift://ilum-hive-metastore:9083</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.sql.catalogImplementation</td>\n",
    "            <td>hive</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td>spark.sql.warehouse.dir</td>\n",
    "            <td>s3a://ilum-data/</td>\n",
    "          </tr>\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>Session-Specific Hive Capabilities:</strong> If Hive is only required for a specific session, configure the necessary environment variables and dependencies on a per-session basis. For example:\n",
    "      <pre><code>{\"conf\": {\"spark.sql.warehouse.dir\": \"s3a://ilum-data/\", \"spark.kubernetes.container.image\": \"ilum/spark:3.5.3-delta\", \"spark.hadoop.hive.metastore.uris\": \"thrift://ilum-hive-metastore:9083\", \"spark.sql.catalogImplementation\": \"hive\"}, \"driverMemory\": \"1000M\", \"executorCores\": 2}</code></pre>\n",
    "      This configuration prepares your session for Delta operations without affecting other workflows.\n",
    "    </li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T11:46:54.983540Z",
     "iopub.status.busy": "2025-03-11T11:46:54.983225Z",
     "iopub.status.idle": "2025-03-11T11:46:54.985804Z",
     "shell.execute_reply": "2025-03-11T11:46:54.985497Z",
     "shell.execute_reply.started": "2025-03-11T11:46:54.983523Z"
    }
   },
   "source": [
    "First, we'll need to load the spark magic extension. You can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ilum's Bundled Jupyter is ready to work out of the box and has a predefined endpoint address, which points to ```livy-proxy```. \n",
    "\n",
    "Use **%manage_spark** to create new session. \n",
    "\n",
    "Choose between Scala or Python, adjust Spark settings if necessary, and then click the `Create Session` button. As simple as that. \n",
    "\n",
    "The following example is written in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start processing, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "   \n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Dedicated Database for the Use Case**\n",
    "\n",
    "A good practice in data engineering is to separate data within dedicated databases for specific use cases. This approach helps maintain data organization and makes it easier to manage, query, and scale.\n",
    "\n",
    "For this use case, we will create a database named `example_bronze`. This will ensure that all data related to this use case is stored in a structured and isolated manner.\n",
    "\n",
    "To create the database, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    spark.sql(\"CREATE DATABASE example_bronze\")\n",
    "    spark.sql(\"USE example_bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:16:17.377528Z",
     "iopub.status.busy": "2024-12-05T14:16:17.376201Z",
     "iopub.status.idle": "2024-12-05T14:16:17.382466Z",
     "shell.execute_reply": "2024-12-05T14:16:17.381376Z",
     "shell.execute_reply.started": "2024-12-05T14:16:17.377470Z"
    }
   },
   "source": [
    "### **Step 2: Load Raw Data**\n",
    "The second step is to push the data into the bronze layer. This is usually done automatically from many different sources, but for this notebook the test data will be loaded manually.\n",
    "\n",
    "Below, each of the three sample data packages is downloaded from a remote repository without any processing.\n",
    "\n",
    "**Animals:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_url = 'https://raw.githubusercontent.com/ilum-cloud/ilum-python-examples/main/animals.csv'\n",
    "    \n",
    "    animals_df = spark.createDataFrame(pd.read_csv(animals_url))\n",
    "    animals_df.printSchema()\n",
    "    animals_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Owners:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    owners_url = 'https://raw.githubusercontent.com/ilum-cloud/ilum-python-examples/main/owners.csv'\n",
    "\n",
    "    owners_df = spark.createDataFrame(pd.read_csv(owners_url))\n",
    "    owners_df.printSchema()\n",
    "    owners_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Species:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "    \n",
    "    species_url = 'https://raw.githubusercontent.com/ilum-cloud/ilum-python-examples/main/species.csv'\n",
    "\n",
    "    species_df = spark.createDataFrame(pd.read_csv(species_url))\n",
    "    species_df.printSchema()\n",
    "    species_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:29:00.592662Z",
     "iopub.status.busy": "2024-12-05T14:29:00.592151Z",
     "iopub.status.idle": "2024-12-05T14:29:00.601806Z",
     "shell.execute_reply": "2024-12-05T14:29:00.599630Z",
     "shell.execute_reply.started": "2024-12-05T14:29:00.592630Z"
    }
   },
   "source": [
    "### **Step 3: Save Data to the Bronze Layer**\n",
    "In this step, we will save the raw data to a dedicated Bronze Layer location. Since Ilum provides integrated S3 storage and Hive integration, no credentials are required to access the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "    animals_df.write.format(\"csv\").saveAsTable(\"animals\")\n",
    "    owners_df.write.format(\"csv\").saveAsTable(\"owners\")\n",
    "    species_df.write.format(\"csv\").saveAsTable(\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Summary**\n",
    "In this example:\n",
    " - **We loaded raw data** from a CSV file containing sales transactions.\n",
    " - **We saved the data in CSV format** to a dedicated Bronze Layer location using Ilum's integrated S3 storage.\n",
    "\n",
    "Storing raw data in the Bronze Layer this way ensures a solid foundation for further processing and analysis in the higher layers of the Medallion architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "\n",
    "Now that youâ€™re done with your work, you should clean them up to free up resources when theyâ€™re no longer in use. \n",
    "Simply click on the Delete buttons!\n",
    "\n",
    "![Ilum session clean](../../images/clean_ilum_jupyter_session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Click here to proceed to the \"Bronze to silver\" section.](2_Bronze_to_silver.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
